{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad73f8d-3150-473e-a882-579c902424eb",
   "metadata": {},
   "source": [
    "## Self-Attention Does Not Reuire O(N^2) Memory\n",
    "\n",
    "Implementation of: https://arxiv.org/pdf/2112.05682.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72717a69-9cf8-4fb7-a3f5-4e6431785ebb",
   "metadata": {},
   "source": [
    "### Example of actual pytorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continental-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linear_mem_attention_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wicked-johnston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'linear_mem_attention_pytorch.linear_mem_attn_torch' from '/Users/ericalcaidealdeano/miniconda3/envs/charm/lib/python3.7/site-packages/linear_mem_attention_pytorch-0.0.1-py3.7.egg/linear_mem_attention_pytorch/linear_mem_attn_torch.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_mem_attention_pytorch.linear_mem_attn_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df95097b-2e37-42fc-aa01-354e8b02f472",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'linear_mem_attn_torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lh/zgndpx8x755_lcsq48lp_5t40000gn/T/ipykernel_50249/1057510761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlinear_mem_attention_torch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlinear_mem_attention_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mqkv2res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/charm/lib/python3.7/site-packages/linear_mem_attention_torch-0.0.1-py3.7.egg/linear_mem_attention_torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlinear_mem_attn_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_mem_attn_torch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlinear_mem_attn_torch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfast_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlinear_mem_attn_torch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Â # this would make the module require jax.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'linear_mem_attn_torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from linear_mem_attention_torch import *\n",
    "from linear_mem_attention_torch.utils import qkv2res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd3fffa-f59d-438d-8af0-e2ae37829b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def qkv2res(q, k, v):  \n",
    "    # return (q @ torch.transpose(k, -1, -2)).softmax(dim=-1) @ v\n",
    "    qk = torch.einsum('b h i d, b h j d -> b h i j', q, k).softmax(dim=-1)\n",
    "    return torch.einsum('b h i j, b h j d -> b h i d', qk, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b392c-d027-43e3-93a5-dc11c97d41ad",
   "metadata": {},
   "source": [
    "### Paper base jax code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdc2a3f-b7cb-488a-883e-52d1f47ba9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, jax, math\n",
    "from jax import numpy as jnp\n",
    "\n",
    "def _query_chunk_attention(query, key, value, precision, key_chunk_size=4096):\n",
    "    \"\"\"Multi-head dot product attention with a limited number of queries.\"\"\"\n",
    "    num_kv, num_heads, k_features = key.shape\n",
    "    v_features = value.shape[-1]\n",
    "    key_chunk_size = min(key_chunk_size, num_kv)\n",
    "    query = query / jnp.sqrt(k_features)\n",
    "\n",
    "    @functools.partial(jax.checkpoint, prevent_cse=False)\n",
    "    def summarize_chunk(query, key, value):\n",
    "        attn_weights = jnp.einsum('qhd,khd->qhk', query, key, precision=precision)\n",
    "        max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n",
    "        max_score = jax.lax.stop_gradient(max_score)\n",
    "        exp_weights = jnp.exp(attn_weights - max_score)\n",
    "        exp_values = jnp.einsum('vhf,qhv->qhf', value, exp_weights, precision=precision)\n",
    "        return (\n",
    "            exp_values, exp_weights.sum(axis=-1),\n",
    "            max_score.reshape((query.shape[0], num_heads))\n",
    "        )\n",
    "\n",
    "    def chunk_scanner(chunk_idx):\n",
    "        key_chunk = jax.lax.dynamic_slice(\n",
    "            key, (chunk_idx, 0, 0),\n",
    "            slice_sizes=(key_chunk_size, num_heads, k_features)\n",
    "        )\n",
    "        value_chunk = jax.lax.dynamic_slice(\n",
    "             value, (chunk_idx, 0, 0),\n",
    "             slice_sizes=(key_chunk_size, num_heads, v_features)\n",
    "        )\n",
    "        return summarize_chunk(query, key_chunk, value_chunk)\n",
    "\n",
    "    chunk_values, chunk_weights, chunk_max = jax.lax.map(\n",
    "        chunk_scanner, xs=jnp.arange(0, num_kv, key_chunk_size)\n",
    "    )\n",
    "\n",
    "    global_max = jnp.max(chunk_max, axis=0, keepdims=True)\n",
    "    max_diffs = jnp.exp(chunk_max - global_max)\n",
    "    chunk_values *= jnp.expand_dims(max_diffs, axis=-1)\n",
    "    chunk_weights *= max_diffs\n",
    "\n",
    "    all_values = chunk_values.sum(axis=0)\n",
    "    all_weights = jnp.expand_dims(chunk_weights, -1).sum(axis=0)\n",
    "    return all_values / all_weights\n",
    "\n",
    "def attention(\n",
    "    query, key, value, precision=jax.lax.Precision.HIGHEST,\n",
    "    query_chunk_size=1024\n",
    "):\n",
    "    \"\"\"Memory-efficient multi-head dot product attention.\"\"\"\n",
    "    num_q, num_heads, q_features = query.shape\n",
    "    def chunk_scanner(chunk_idx, _):\n",
    "        query_chunk = jax.lax.dynamic_slice(\n",
    "             query, (chunk_idx, 0, 0),\n",
    "             slice_sizes=(min(query_chunk_size, num_q), num_heads, q_features)\n",
    "        )\n",
    "        return (\n",
    "            chunk_idx + query_chunk_size,\n",
    "            _query_chunk_attention(query_chunk, key, value, precision=precision)\n",
    "        )\n",
    "\n",
    "    _, res = jax.lax.scan(\n",
    "    chunk_scanner, init=0, xs=None, length=math.ceil(num_q / query_chunk_size))\n",
    "    return res.reshape(num_q, num_heads, value.shape[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989397fe-37ab-4196-91ac-1baf95f80d86",
   "metadata": {},
   "source": [
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737d42eb-7f2c-492c-b3b4-bd5ccd17f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Any, List\n",
    "from types import FunctionType\n",
    "\n",
    "@torch.jit.script\n",
    "def dynamic_slice(\n",
    "    x: torch.Tensor, \n",
    "    slices: Tuple[int, int, int], \n",
    "    slice_sizes: Tuple[int, int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" approx like jax.lax.dynamic_slice.\n",
    "        * NOTE: assumes we dont work on first dim\n",
    "        Ex: \n",
    "        dynamic_slice(\n",
    "            x, \n",
    "            slices=(0, 0, 0),\n",
    "            slice_sizes=(16, 64, 64)\n",
    "        )\n",
    "    \"\"\"\n",
    "    return x[\n",
    "        :,\n",
    "        slices[0]: slices[0] + slice_sizes[0],\n",
    "        slices[1]: slices[1] + slice_sizes[1],\n",
    "        slices[2]: slices[2] + slice_sizes[2],\n",
    "    ]\n",
    "\n",
    "def torch_map(fn, xs) -> Tuple[torch.Tensor, torch.Tensor,torch.Tensor]:\n",
    "    \"\"\" approx like jax.lax.map \"\"\"\n",
    "    return\n",
    "\n",
    "def torch_scan(\n",
    "        f: FunctionType,\n",
    "        init: int = 0,\n",
    "        xs: Optional[List] = None,\n",
    "        length: int = 0\n",
    ") -> Tuple[Any, torch.Tensor]:\n",
    "        if xs is None:\n",
    "            xs = [None] * length\n",
    "        carry = init\n",
    "        ys = []\n",
    "        for x in xs:\n",
    "            carry, y = f(carry, x)\n",
    "            ys.append(y)\n",
    "        return carry, torch.stack(ys, dim=0)\n",
    "\n",
    "###################\n",
    "## ADAPTED FROM: https://arxiv.org/pdf/2112.05682.pdf\n",
    "###################\n",
    "\n",
    "def torch_query_chunk_attention(query, key, value, key_chunk_size=4096):\n",
    "    \"\"\"Multi-head dot product attention with a limited number of queries.\"\"\"\n",
    "    batch, num_kv, num_heads, k_features = key.shape\n",
    "    v_features = value.shape[-1]\n",
    "    query_chunk = query.shape[1] # b n h d\n",
    "    key_chunk_size = min(key_chunk_size, num_kv)\n",
    "    query = query / k_features**0.5\n",
    "\n",
    "    # @functools.partial(jax.checkpoint, prevent_cse=False)\n",
    "    def summarize_chunk(\n",
    "        query: torch.Tensor, key: torch.Tensor, value: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        attn_weights = torch.einsum('bqhd,bkhd->bqhk', query, key)\n",
    "        max_score = torch.amax(attn_weights, dim=-1, keepdim=True).detach()\n",
    "        exp_weights = torch.exp(attn_weights - max_score)\n",
    "        exp_values = torch.einsum('bvhf,bqhv->bqhf', value, exp_weights)\n",
    "        # (b q h f), (b q h), (b q h 1)\n",
    "        return exp_values, exp_weights.sum(dim=-1), max_score  \n",
    "\n",
    "    def chunk_scanner(\n",
    "        chunk_idx: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        key_chunk = dynamic_slice(key, (chunk_idx, 0, 0),\n",
    "            slice_sizes=(key_chunk_size, num_heads, k_features)\n",
    "        )\n",
    "        value_chunk = dynamic_slice(\n",
    "            value, (chunk_idx, 0, 0),\n",
    "            slice_sizes=(key_chunk_size, num_heads, v_features)\n",
    "        )\n",
    "        return summarize_chunk(query, key_chunk, value_chunk)\n",
    "\n",
    "    chunk_iter = np.arange(0, num_kv, key_chunk_size)\n",
    "    chunk_values = torch.zeros(len(chunk_iter), batch, query_chunk, num_heads, v_features).to(query)\n",
    "    chunk_weights = torch.zeros(len(chunk_iter), batch, query_chunk, num_heads).to(query)\n",
    "    chunk_max = torch.zeros(len(chunk_iter), batch, query_chunk, num_heads, 1).to(query)\n",
    "    for i, xi in enumerate(chunk_iter):\n",
    "        chunk_values[i], chunk_weights[i], chunk_max[i] = chunk_scanner(xi)\n",
    "\n",
    "    global_max = torch.amax(chunk_max, dim=0, keepdim=True)\n",
    "    max_diffs = torch.exp(chunk_max - global_max)\n",
    "\n",
    "    chunk_values *= max_diffs\n",
    "    chunk_weights *= max_diffs[..., 0]\n",
    "\n",
    "    all_values = chunk_values.sum(dim=0)\n",
    "    all_weights = torch.unsqueeze(chunk_weights, -1).sum(dim=0)\n",
    "    return all_values / all_weights\n",
    "\n",
    "def torch_attention(\n",
    "    query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, \n",
    "    query_chunk_size=1024, key_chunk_size=4096,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Memory-efficient multi-head dot product attention. \n",
    "        qkv should be provided in ()\n",
    "    \"\"\"\n",
    "    batch, num_q, num_heads, q_features = query.shape\n",
    "\n",
    "    def chunk_scanner(chunk_idx: int, _):\n",
    "        query_chunk = dynamic_slice(\n",
    "            query, (chunk_idx, 0, 0),\n",
    "            slice_sizes=(min(query_chunk_size, num_q), num_heads, q_features)\n",
    "        )\n",
    "        return (\n",
    "            chunk_idx + query_chunk_size,\n",
    "            torch_query_chunk_attention(query_chunk, key, value, key_chunk_size=key_chunk_size)\n",
    "        )\n",
    "\n",
    "    _, res = torch_scan(chunk_scanner, init=0, xs=None, length=np.math.ceil(num_q / query_chunk_size))\n",
    "    return res.reshape(batch, num_q, num_heads, value.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6532-6782-48e7-abfc-528cb7054bf0",
   "metadata": {},
   "source": [
    "#### Tests for Pytorch Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5addef63-6210-4a29-a54f-d2c5d8f3c485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16384, 64]),\n",
       " (16384, 1, 64),\n",
       " torch.Size([1, 1, 16384, 64]),\n",
       " torch.Size([1, 16384, 1, 64]),\n",
       " torch.Size([2, 16384, 1, 64]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, L, D = 1, 2**14, 64\n",
    "a = torch.randn(B, L, D) # .cuda()\n",
    "b = a[:, None, :, :]                                           # (b h n d) batch and heads\n",
    "a_ = jax.numpy.asarray(torch.transpose(a, 0, 1).cpu().numpy()) # (n h d) heads but not batch\n",
    "b_ = torch.from_numpy( np.asarray(a_) )[None, ...]\n",
    "c_ = torch.cat([b_, b_], dim=0)\n",
    "a.shape, a_.shape, b.shape, b_.shape, c_.shape\n",
    "# attn = Attention(D)\n",
    "# %timeit attn(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd5af9d-df50-46fc-af9d-14c3ada0300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batching works\n",
    "assert torch.allclose(\n",
    "    torch_attention(b_, b_, b_)[0], # .shape b n h d\n",
    "    torch_attention(c_, c_, c_)[0], # .shape b n h d\n",
    "), \"Batching does not work\"\n",
    "\n",
    "# test query chunking works\n",
    "assert torch.allclose(\n",
    "    torch_attention(b_, b_, b_, query_chunk_size=32)[0], # .shape b n h d\n",
    "    torch_attention(b_, b_, b_)[0], # .shape b n h d\n",
    "    atol = 1e-6\n",
    "), \"Query chunking does not work\"\n",
    "\n",
    "# test key chunking works\n",
    "assert torch.allclose(\n",
    "    torch_attention(b_, b_, b_, key_chunk_size=128)[0], # .shape b n h d\n",
    "    torch_attention(b_, b_, b_)[0], # .shape b n h d\n",
    "    atol = 1e-6\n",
    "), \"Key chunking does not work\"\n",
    "\n",
    "# test correctness chunking works\n",
    "assert torch.allclose(\n",
    "    torch_attention(b_, b_, b_)[0], # .shape b n h d\n",
    "    torch.transpose( qkv2res(*[torch.transpose(b_, 1, 2)]*3), 1, 2 )[0], # .shape b n h d\n",
    "    atol = 1e+1 # slight difference, but paper code shows it as well\n",
    "), \"Key chunking does not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40d3e9-2454-472c-adb4-862aa6110f7c",
   "metadata": {},
   "source": [
    "#### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f3eed1-49a5-487e-b36d-a3fc0f613627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06409f78-b21c-4575-b346-9be5ee59afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_attn = jax.jit(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8770b-77a2-4296-bf2d-b0645f1bf036",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=64\n",
      "-> jax compiled linear\n",
      "133 Âµs Â± 11.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "439 Âµs Â± 378 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "182 Âµs Â± 387 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=128\n",
      "-> jax compiled linear\n",
      "129 Âµs Â± 8.91 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "534 Âµs Â± 940 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "581 Âµs Â± 405 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=256\n",
      "-> jax compiled linear\n",
      "151 Âµs Â± 8.36 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "893 Âµs Â± 810 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "1.53 ms Â± 304 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=512\n",
      "-> jax compiled linear\n",
      "138 Âµs Â± 10.2 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "2.07 ms Â± 1.58 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "7.46 ms Â± 947 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=1024\n",
      "-> jax compiled linear\n",
      "176 Âµs Â± 10.7 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "6.72 ms Â± 900 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "30.4 ms Â± 98.3 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=2048\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 10:59:53.412695: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386 Âµs Â± 15.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "25.6 ms Â± 72.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "128 ms Â± 196 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=4096\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 11:00:06.283274: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 ms Â± 14 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "114 ms Â± 8.81 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "576 ms Â± 151 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=8192\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 11:00:20.789814: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.11 ms Â± 18.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "465 ms Â± 21.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "2.36 s Â± 640 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=16384\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 11:00:43.850223: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6 ms Â± 382 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "1.74 s Â± 21.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "9.47 s Â± 4.55 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=32768\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 11:02:14.167685: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.8 ms Â± 2.42 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "7.92 s Â± 501 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "37.3 s Â± 21.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=65536\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 11:08:16.559469: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 ms Â± 670 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "37.5 s Â± 467 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n"
     ]
    }
   ],
   "source": [
    "for exp2 in range(6, 16+1): \n",
    "    B, L, D = 1, 2**exp2, 64\n",
    "    a = torch.randn(B, L, D) # .cuda()\n",
    "    b = a[:, None, :, :]                                           # (b h n d) batch and heads\n",
    "    a_ = jax.numpy.asarray(torch.transpose(a, 0, 1).cpu().numpy()) # (n h d) heads but not batch\n",
    "    b_ = torch.from_numpy( np.asarray(a_) )[None, ...]\n",
    "    a.shape, a_.shape, b.shape, b_.shape\n",
    "    # attn = Attention(D)\n",
    "    # %timeit attn(a, a)\n",
    "    print()\n",
    "    print(f\"Attn w/ heads=1, batch=1, D=64, For L={2**exp2}\")\n",
    "    print(\"-> jax compiled linear\")\n",
    "    %timeit jax_attn(a_, a_, a_).block_until_ready()\n",
    "    print(\"-> torch linear\")\n",
    "    %timeit torch_attention(b_, b_, b_)\n",
    "    print(\"-> torch standard (einsum is used, but similar to matmul)\")\n",
    "    %timeit qkv2res(b, b, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0c015-4b7f-4aba-8187-79703f940edd",
   "metadata": {},
   "source": [
    "### Compare to community implementation (L = 2**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5e6ea6-2beb-4164-bba9-05e4e047dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_efficient_attention import efficient_dot_product_attention_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f71901-7eb1-4d44-ae24-56912c7291d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "2.33 s Â± 116 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit efficient_dot_product_attention_pt(b_, b_, b_, key_chunk_size=1024, query_chunk_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8743b014-6272-4cf2-8213-4ef706bbe00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74 s Â± 12.5 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch_attention(b_, b_, b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af1325-2f5c-421d-9d25-dab94e189504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a118ef8-a2b8-4789-a8f9-39b49469ec80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charm",
   "language": "python",
   "name": "charm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
