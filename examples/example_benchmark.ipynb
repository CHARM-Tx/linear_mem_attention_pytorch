{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad73f8d-3150-473e-a882-579c902424eb",
   "metadata": {},
   "source": [
    "## Self-Attention Does Not Reuire O(N^2) Memory\n",
    "\n",
    "Implementation of: https://arxiv.org/pdf/2112.05682.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "played-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72717a69-9cf8-4fb7-a3f5-4e6431785ebb",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continental-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import jax\n",
    "\n",
    "from linear_mem_attention_pytorch import *\n",
    "from linear_mem_attention_pytorch.utils import qkv2res\n",
    "from linear_mem_attention_pytorch.fast_attn import Attention\n",
    "from linear_mem_attention_pytorch import linear_mem_attn_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6532-6782-48e7-abfc-528cb7054bf0",
   "metadata": {},
   "source": [
    "#### Tests for Pytorch Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5addef63-6210-4a29-a54f-d2c5d8f3c485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 64]),\n",
       " (64, 1, 64),\n",
       " torch.Size([1, 1, 64, 64]),\n",
       " torch.Size([1, 64, 1, 64]),\n",
       " torch.Size([2, 64, 1, 64]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, L, D = 1, 2**6, 64\n",
    "a = torch.randn(B, L, D) # .cuda()\n",
    "b = a[:, None, :, :]                                           # (b h n d) batch and heads\n",
    "d = torch.transpose(a, 0, 1).contiguous()\n",
    "a_ = jax_dlpack.from_dlpack(torch_dlpack.to_dlpack( d )) # (n h d) heads but not batch\n",
    "a_ = jax.device_put(a_)\n",
    "b_ = d[None, ...]\n",
    "c_ = torch.cat([b_, b_], dim=0)\n",
    "a.shape, a_.shape, b.shape, b_.shape, c_.shape\n",
    "# attn = Attention(D)\n",
    "# %timeit attn(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cceed461-5781-458a-bf9c-b242b4acaeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_attn = jax.jit(linear_mem_attn_jax.attention)\n",
    "jax_attn(a_, a_, a_)\n",
    "attention(b_, b_, b_)\n",
    "qkv2res(b_, b_, b_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40d3e9-2454-472c-adb4-862aa6110f7c",
   "metadata": {},
   "source": [
    "#### Performance Comparison\n",
    "#### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db8770b-77a2-4296-bf2d-b0645f1bf036",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=64\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:20:23.396989: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2021-12-27 19:20:23.440659: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2021-12-27 19:20:23.631694: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 µs ± 16.8 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "540 µs ± 453 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "226 µs ± 213 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=128\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:20:30.158588: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 µs ± 16.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "642 µs ± 443 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "584 µs ± 792 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=256\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:20:40.421311: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 µs ± 12.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "999 µs ± 950 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "2.42 ms ± 1.31 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=512\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:20:50.819194: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 µs ± 14 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "2.12 ms ± 921 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "8.29 ms ± 1.15 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=1024\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:20:59.602680: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 µs ± 15.8 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "6.68 ms ± 8.17 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "31.9 ms ± 8.68 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=2048\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:21:07.968591: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "25.4 ms ± 81.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "128 ms ± 132 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=4096\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:21:20.782667: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24 ms ± 14.7 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "138 ms ± 4.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "560 ms ± 476 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=8192\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:21:36.845905: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.08 ms ± 21.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "440 ms ± 15.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "2.37 s ± 745 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=16384\n",
      "-> jax compiled linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 19:21:59.804519: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.0.145, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5 ms ± 31.8 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "1.75 s ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "9.64 s ± 11.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for exp2 in range(6, 14+1): \n",
    "    B, L, D = 1, 2**exp2, 64\n",
    "    a = torch.randn(B, L, D) # .cuda()\n",
    "    b = a[:, None, :, :]                                           # (b h n d) batch and heads\n",
    "    a_ = jax.numpy.asarray(torch.transpose(a, 0, 1).cpu().numpy()) # (n h d) heads but not batch\n",
    "    b_ = torch.from_numpy( np.asarray(a_) )[None, ...]\n",
    "    a.shape, a_.shape, b.shape, b_.shape\n",
    "    # attn = Attention(D)\n",
    "    # %timeit attn(a, a)\n",
    "    print()\n",
    "    print(f\"Attn w/ heads=1, batch=1, D=64, For L={2**exp2}\")\n",
    "    print(\"-> jax compiled linear\")\n",
    "    %timeit jax_attn(a_, a_, a_).block_until_ready()\n",
    "    print(\"-> torch linear\")\n",
    "    %timeit attention(b_, b_, b_, query_chunk_size=1024, key_chunk_size=4096)\n",
    "    print(\"-> torch standard (einsum is used, but similar to matmul)\")\n",
    "    %timeit qkv2res(b_, b_, b_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ffb09-8c5c-4995-9ad5-8746b2210f1d",
   "metadata": {},
   "source": [
    "#### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd91c49-24fa-440c-92ea-76edc1147aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import dlpack as torch_dlpack\n",
    "from jax import dlpack as jax_dlpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f790622-7a73-44d4-93ac-e60ab5c0e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_torch_gpu(fn, b_): \n",
    "    fn(b_, b_, b_)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ffd913a-305f-4fb1-a789-97fb61745ea3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=64\n",
      "-> jax compiled linear\n",
      "54.8 µs ± 111 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "1.01 ms ± 943 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "147 µs ± 257 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=128\n",
      "-> jax compiled linear\n",
      "193 µs ± 20 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "1.03 ms ± 1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "150 µs ± 284 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=256\n",
      "-> jax compiled linear\n",
      "938 µs ± 47.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "1.1 ms ± 387 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "187 µs ± 98.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=512\n",
      "-> jax compiled linear\n",
      "2.22 ms ± 301 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "1.13 ms ± 871 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "183 µs ± 487 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=1024\n",
      "-> jax compiled linear\n",
      "7.5 ms ± 929 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "1.11 ms ± 5.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "173 µs ± 2.41 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=2048\n",
      "-> jax compiled linear\n",
      "39.7 ms ± 9.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "2.13 ms ± 7.61 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "229 µs ± 283 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=4096\n",
      "-> jax compiled linear\n",
      "161 ms ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "4.19 ms ± 3.62 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "650 µs ± 609 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=8192\n",
      "-> jax compiled linear\n",
      "639 ms ± 37.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch linear\n",
      "13.9 ms ± 6.67 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/ubuntu/linear_mem_attention_pytorch/linear_mem_attention_pytorch/utils.py\", line 10, in qkv2res\n    \"\"\" Inputs must be in (b n h d) format. \"\"\"  \n    # return (q @ torch.transpose(k, -1, -2)).softmax(dim=-1) @ v\n    qk = torch.einsum('b i h d, b j h d -> b i h j', q, k).softmax(dim=-1)\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return torch.einsum('b i h j, b j h d -> b i h d', qk, v)\nRuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.78 GiB total capacity; 262.00 MiB already allocated; 218.75 MiB free; 272.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16400/3742571224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wrap_torch_gpu(attention, b_) #\\xa0 attention(b_, b_, b_, query_chunk_size=1024, key_chunk_size=4096); torch.cuda.synchronize()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-> torch standard (einsum is used, but similar to matmul)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wrap_torch_gpu(qkv2res, b_) # qkv2res(b_, b_, b_); torch.cuda.synchronize()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch-gpu/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2362\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2365\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-gpu/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-gpu/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-gpu/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-gpu/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16400/2048882575.py\u001b[0m in \u001b[0;36mwrap_torch_gpu\u001b[0;34m(fn, b_)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrap_torch_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/ubuntu/linear_mem_attention_pytorch/linear_mem_attention_pytorch/utils.py\", line 10, in qkv2res\n    \"\"\" Inputs must be in (b n h d) format. \"\"\"  \n    # return (q @ torch.transpose(k, -1, -2)).softmax(dim=-1) @ v\n    qk = torch.einsum('b i h d, b j h d -> b i h j', q, k).softmax(dim=-1)\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return torch.einsum('b i h j, b j h d -> b i h d', qk, v)\nRuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.78 GiB total capacity; 262.00 MiB already allocated; 218.75 MiB free; 272.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "for exp2 in range(6, 14+1): \n",
    "    B, L, D = 1, 2**exp2, 64\n",
    "    a = torch.randn(B, L, D).cuda()\n",
    "    b = a[:, None, :, :]                                           # (b h n d) batch and heads\n",
    "    d = torch.transpose(a, 0, 1).contiguous()\n",
    "    a_ = jax_dlpack.from_dlpack(torch_dlpack.to_dlpack( d.cpu() )) # (n h d) heads but not batch\n",
    "    a_ = jax.device_put(a_)\n",
    "    b_ = d[None, ...]\n",
    "    c_ = torch.cat([b_, b_], dim=0)\n",
    "    a.shape, a_.shape, b.shape, b_.shape, c_.shape\n",
    "    # attn = Attention(D)\n",
    "    # %timeit attn(a, a)\n",
    "    print()\n",
    "    print(f\"Attn w/ heads=1, batch=1, D=64, For L={2**exp2}\")\n",
    "    print(\"-> jax compiled linear\")\n",
    "    %timeit jax_attn(a_, a_, a_).block_until_ready()\n",
    "    print(\"-> torch linear\")\n",
    "    %timeit wrap_torch_gpu(attention, b_) #  attention(b_, b_, b_, query_chunk_size=1024, key_chunk_size=4096); torch.cuda.synchronize()\n",
    "    print(\"-> torch standard (einsum is used, but similar to matmul)\")\n",
    "    %timeit wrap_torch_gpu(qkv2res, b_) # qkv2res(b_, b_, b_); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0c015-4b7f-4aba-8187-79703f940edd",
   "metadata": {},
   "source": [
    "### Compare to community implementation (L = 2**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5e6ea6-2beb-4164-bba9-05e4e047dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_efficient_attention import efficient_dot_product_attention_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd336cdc-38c2-45ee-9450-9341fc435474",
   "metadata": {},
   "source": [
    "#### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f71901-7eb1-4d44-ae24-56912c7291d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community implementation\n",
      "0\n",
      "4096\n",
      "0\n",
      "4096\n",
      "0\n",
      "4096\n",
      "0\n",
      "4096\n",
      "0\n",
      "4096\n",
      "0\n",
      "4096\n",
      "0\n",
      "4096\n",
      "0\n",
      "4096\n",
      "575 ms ± 16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Community implementation\")\n",
    "%timeit efficient_dot_product_attention_pt(*[b_.cpu()]*3, key_chunk_size=1024, query_chunk_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8743b014-6272-4cf2-8213-4ef706bbe00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "488 ms ± 32.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\")\n",
    "%timeit attention( *[b_.cpu()]*3 , key_chunk_size=1024, query_chunk_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d2a34-1271-4117-b275-67cde255fd8a",
   "metadata": {},
   "source": [
    "#### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a118ef8-a2b8-4789-a8f9-39b49469ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community implementation\n",
      "0\n",
      "Failed. See error (uncomment the try/except for more info):\n",
      "\n",
      "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)\n"
     ]
    }
   ],
   "source": [
    "print(\"Community implementation\")\n",
    "try: \n",
    "    %timeit efficient_dot_product_attention_pt(b_, b_, b_, key_chunk_size=1024, query_chunk_size=4096)\n",
    "except Exception as e: \n",
    "    print(\"Failed. See error (uncomment the try/except for more info):\\n\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a3e29e-f437-4044-a3b1-e0f7b3ff3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "11.9 ms ± 7.42 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\")\n",
    "%timeit attention(b_, b_, b_, key_chunk_size=1024, query_chunk_size=4096)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
