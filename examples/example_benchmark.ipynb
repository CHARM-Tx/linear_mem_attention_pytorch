{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad73f8d-3150-473e-a882-579c902424eb",
   "metadata": {},
   "source": [
    "## Self-Attention Does Not Reuire O(N^2) Memory\n",
    "\n",
    "Implementation of: https://arxiv.org/pdf/2112.05682.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "played-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72717a69-9cf8-4fb7-a3f5-4e6431785ebb",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continental-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import jax\n",
    "\n",
    "from linear_mem_attention_pytorch import *\n",
    "from linear_mem_attention_pytorch.utils import qkv2res\n",
    "from linear_mem_attention_pytorch.fast_attn import Attention\n",
    "from linear_mem_attention_pytorch import linear_mem_attn_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6532-6782-48e7-abfc-528cb7054bf0",
   "metadata": {},
   "source": [
    "#### Tests for Pytorch Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5addef63-6210-4a29-a54f-d2c5d8f3c485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16384, 64]),\n",
       " (16384, 1, 64),\n",
       " torch.Size([1, 1, 16384, 64]),\n",
       " torch.Size([1, 16384, 1, 64]),\n",
       " torch.Size([2, 16384, 1, 64]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, L, D = 1, 2**14, 64\n",
    "a = torch.randn(B, L, D) # .cuda()\n",
    "b = a[:, None, :, :]                                           # (b h n d) batch and heads\n",
    "a_ = jax.numpy.asarray(torch.transpose(a, 0, 1).cpu().numpy()) # (n h d) heads but not batch\n",
    "b_ = torch.from_numpy( np.asarray(a_) )[None, ...]\n",
    "c_ = torch.cat([b_, b_], dim=0)\n",
    "a.shape, a_.shape, b.shape, b_.shape, c_.shape\n",
    "# attn = Attention(D)\n",
    "# %timeit attn(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40d3e9-2454-472c-adb4-862aa6110f7c",
   "metadata": {},
   "source": [
    "#### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f3d9e7-b0c3-4dcf-b1b0-9a34f6b31360",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def qkv2res2(q, k, v): \n",
    "    \"\"\" Inputs must be in (b n h d) format. \"\"\"  \n",
    "    # return (q @ torch.transpose(k, -1, -2)).softmax(dim=-1) @ v\n",
    "    qk = torch.einsum('b h i d, b h j d -> b h i j', q, k).softmax(dim=-1)\n",
    "    return torch.einsum('b h i j, b h j d -> b h i d', qk, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06409f78-b21c-4575-b346-9be5ee59afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_attn = jax.jit(linear_mem_attn_jax.attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db8770b-77a2-4296-bf2d-b0645f1bf036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=64\n",
      "-> jax compiled linear\n",
      "135 µs ± 5.39 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "535 µs ± 327 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "194 µs ± 58.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=128\n",
      "-> jax compiled linear\n",
      "149 µs ± 5.69 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "638 µs ± 630 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "653 µs ± 213 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=256\n",
      "-> jax compiled linear\n",
      "153 µs ± 6.81 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "997 µs ± 219 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "2.03 ms ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=512\n",
      "-> jax compiled linear\n",
      "156 µs ± 3.94 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "2.11 ms ± 1.93 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "8.04 ms ± 8.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=1024\n",
      "-> jax compiled linear\n",
      "170 µs ± 506 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "-> torch linear\n",
      "6.64 ms ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "29.5 ms ± 86 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=2048\n",
      "-> jax compiled linear\n",
      "392 µs ± 2.04 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch linear\n",
      "26.2 ms ± 285 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "125 ms ± 334 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=4096\n",
      "-> jax compiled linear\n",
      "1.14 ms ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "-> torch linear\n",
      "111 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "545 ms ± 1.82 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=8192\n",
      "-> jax compiled linear\n",
      "4.52 ms ± 2.17 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch linear\n",
      "504 ms ± 48.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "2.32 s ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Attn w/ heads=1, batch=1, D=64, For L=16384\n",
      "-> jax compiled linear\n",
      "17.3 ms ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "-> torch linear\n",
      "1.81 s ± 28.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "-> torch standard (einsum is used, but similar to matmul)\n",
      "9.71 s ± 12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "for exp2 in range(6, 14+1): \n",
    "    B, L, D = 1, 2**exp2, 64\n",
    "    a = torch.randn(B, L, D) # .cuda()\n",
    "    b = a[:, None, :, :]                                           # (b h n d) batch and heads\n",
    "    a_ = jax.numpy.asarray(torch.transpose(a, 0, 1).cpu().numpy()) # (n h d) heads but not batch\n",
    "    b_ = torch.from_numpy( np.asarray(a_) )[None, ...]\n",
    "    a.shape, a_.shape, b.shape, b_.shape\n",
    "    # attn = Attention(D)\n",
    "    # %timeit attn(a, a)\n",
    "    print()\n",
    "    print(f\"Attn w/ heads=1, batch=1, D=64, For L={2**exp2}\")\n",
    "    print(\"-> jax compiled linear\")\n",
    "    %timeit jax_attn(a_, a_, a_).block_until_ready()\n",
    "    print(\"-> torch linear\")\n",
    "    %timeit attention(b_, b_, b_, query_chunk_size=1024, key_chunk_size=4096)\n",
    "    print(\"-> torch standard (einsum is used, but similar to matmul)\")\n",
    "    %timeit qkv2res(b_, b_, b_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0c015-4b7f-4aba-8187-79703f940edd",
   "metadata": {},
   "source": [
    "### Compare to community implementation (L = 2**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d5e6ea6-2beb-4164-bba9-05e4e047dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_efficient_attention import efficient_dot_product_attention_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13f71901-7eb1-4d44-ae24-56912c7291d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community implementation\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "0\n",
      "4096\n",
      "8192\n",
      "12288\n",
      "2.55 s ± 63.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Community implementation\")\n",
    "%timeit efficient_dot_product_attention_pt(b_, b_, b_, key_chunk_size=1024, query_chunk_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8743b014-6272-4cf2-8213-4ef706bbe00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation\n",
      "1.88 s ± 92.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Our implementation\")\n",
    "%timeit attention(b_, b_, b_, key_chunk_size=1024, query_chunk_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af1325-2f5c-421d-9d25-dab94e189504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a118ef8-a2b8-4789-a8f9-39b49469ec80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
