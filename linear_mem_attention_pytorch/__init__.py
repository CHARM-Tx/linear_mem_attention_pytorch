from linear_mem_attention_pytorch.linear_mem_attn_torch import *
from linear_mem_attention_pytorch import fast_attn
from linear_mem_attention_pytorch import utils

# # this would make the module require jax.
# #Â better do a custom import
# from linear_mem_attn_pytorch import linear_mem_attn_jax as ref_jax_attn
